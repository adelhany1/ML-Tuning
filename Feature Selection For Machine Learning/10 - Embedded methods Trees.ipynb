{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678617d2",
   "metadata": {},
   "source": [
    "EMBEDDED METHODS: \n",
    "# FEATURE SELECTION BY TREE DERIVED VARIABLE IMPORTANCE\n",
    "\n",
    "- DECISION TREES\n",
    "   - Most popular machine learning algorithms\n",
    "   - Highly accurate\n",
    "   - Good generalisation (low overfitting)\n",
    "   - Interpretability\n",
    "- RANDOM FOREST IMPORTANCE\n",
    "   - Random Forests consist of several hundreds of individual decision trees\n",
    "   - The impurity decrease for each feature is averaged across trees\n",
    "- Limitations of RANDOM FOREST\n",
    "   - Correlated features show equal or similar importance\n",
    "   - Correlated features importance is lower than the real importance, determined when tree is built in absence of correlated counterparts\n",
    "   - Highly cardinal variables show greater importance (trees are biased to this type of variables)\n",
    "   \n",
    "- RANDOM FOREST IMPORTANCE:\n",
    "- Build a random forest\n",
    "- Determine feature importance\n",
    "- Select the features with highest importance, There is a scikit-learn implementation for this\n",
    "\n",
    "Recursive feature elimination:\n",
    "\n",
    "- Build random forests\n",
    "- Calculate feature importance\n",
    "- Remove least important feature\n",
    "- Repeat till a condition is met\n",
    "- If the feature removed is correlated to another feature in the dataset, by removing the correlated feature, the true importance of the other feature will be revealed > its importance will increase\n",
    "\n",
    "\n",
    "GRADIENT BOOSTED TREES FEATURE IMPORTANCE\n",
    "\n",
    "- Feature importance calculated in the same way\n",
    "- Biased to highly cardinal features\n",
    "- Importance is susceptible to correlated features\n",
    "- Interpretability of feature importance is not so straightforward:\n",
    "- Later trees fit to the errors of the first trees, therefore feature importance is not necessarily proportional on the influence of the feature on the outcome, rather on the mistakes of the previous trees.\n",
    "- Averaging across trees may not add much information on true relation between feature and target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b7d62",
   "metadata": {},
   "source": [
    "## Random Forest importance\n",
    "\n",
    "Random forests is one the most popular machine learning algorithms. It is so successful because it provide good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
    "\n",
    "Random forests consist typically of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or a combination of features. At each node (that is, at each question), the three divides the dataset in 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how \"pure\" each of the buckets is. \n",
    "\n",
    "For classification, the measure of impurity is either Gini or the entropy. For regression the  measure of impurity is the variance. When training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease elicited by each feature is averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "In general, features that are selected at the top of the trees are more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
    "\n",
    "**Note**\n",
    "- Random Forests and decision trees in general give preference to features with high cardinality\n",
    "- Correlated features will be given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts.\n",
    "\n",
    "I will demonstrate how to select features based on tree importance using a regression and classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a21b0",
   "metadata": {},
   "source": [
    "## Recursive Feature Selection using Random Forests importance\n",
    "\n",
    "Random Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n",
    "\n",
    "Therefore, instead of eliminating features based on importance by brute force like we did in the previous notebook, we could get a better selection by removing one feature at a time, and recalculating the importance on each round. This procedure is called Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE is a hybrid between embedded and wrapper methods: it is based on computation derived when fitting the model, but it also requires fitting several models.\n",
    "\n",
    "The cycle is as follows:\n",
    "\n",
    "- Build Random Forests using all features\n",
    "- Remove least important feature\n",
    "- Build Random Forests and recalculate importance\n",
    "- Repeat until a criteria is met\n",
    "\n",
    "In this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better feature space selection. On the downside, building several Random Forests is quite time and compute resource consuming, in particular if the dataset contains a high number of features.\n",
    "\n",
    "I will demonstrate how to select features based Random Forests importance recursively using sklearn on a classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9e8d0",
   "metadata": {},
   "source": [
    "## Feature selection with decision trees, review\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9682f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9c4952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 301)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset_1.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e209ac02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 300), (15000, 300))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( data.drop(labels=['target'], axis=1), data['target'], test_size=0.3, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55c9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I keep a copy of the dataset with all the variables to compare the performance of machine learning models at the end of the notebook\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7eed525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 266), (15000, 266))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove constant features\n",
    "constant_features = [  feat for feat in X_train.columns if X_train[feat].std() == 0 ]\n",
    "\n",
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0684b7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove quasi-constant features\n",
    "sel = VarianceThreshold(threshold=0.01)  \n",
    "\n",
    "sel.fit(X_train)  # fit finds the features with low variance\n",
    "\n",
    "sum(sel.get_support()) # how many not quasi-constant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a24b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = X_train.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5a1d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 215), (15000, 215))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the features\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b2614f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn transformations lead to numpy arrays here we convert to dataframe:\n",
    "\n",
    "X_train= pd.DataFrame(X_train)\n",
    "X_train.columns = features_to_keep\n",
    "\n",
    "X_test= pd.DataFrame(X_test)\n",
    "X_test.columns = features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8336f2d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicated features\n",
    "# check for duplicated features in the training set\n",
    "duplicated_feat = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    if i % 10 == 0:  # this helps me understand how the loop is going\n",
    "        print(i)\n",
    "\n",
    "    col_1 = X_train.columns[i]\n",
    "\n",
    "    for col_2 in X_train.columns[i + 1:]:\n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(col_2)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4260a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 205), (15000, 205))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicated features\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d2435b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I keep a copy of the dataset except constant, quasi-constant and duplicated variables to measure the performance of machine learning models at the end of the notebook\n",
    "\n",
    "X_train_basic_filter = X_train.copy()\n",
    "X_test_basic_filter = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "866ed614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  93\n"
     ]
    }
   ],
   "source": [
    "# Remove correlated features\n",
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0f58a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 112), (15000, 112))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30a6b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the dataset at  this stage\n",
    "X_train_corr = X_train.copy()\n",
    "X_test_corr = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ba4bb",
   "metadata": {},
   "source": [
    "### Select Features by Random Forests Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a664c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features using the impotance derived from random forests\n",
    "\n",
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=50, random_state=10))\n",
    "sel_.fit(X_train, y_train)\n",
    "\n",
    "# remove features with zero coefficient from dataset and parse again as dataframe (output of sklearn is numpy array)\n",
    "X_train_rf = pd.DataFrame(sel_.transform(X_train))\n",
    "X_test_rf = pd.DataFrame(sel_.transform(X_test))\n",
    "\n",
    "# add the columns name\n",
    "X_train_rf.columns = X_train.columns[(sel_.get_support())]\n",
    "X_test_rf.columns = X_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c165866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 16), (15000, 16))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rf.shape, X_test_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a67b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance in machine learning algorithms\n",
    "# create a function to build random forests and compare performance in train and test set\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)   \n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))    \n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9b0d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.807612232524249\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7868832427636059\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "run_randomForests(X_train_original,  X_test_original, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87b04a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.810290026780428\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7914020645941601\n"
     ]
    }
   ],
   "source": [
    "# filter methods - basic\n",
    "run_randomForests(X_train_basic_filter, X_test_basic_filter, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e42f7534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8066004772684517\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7859521124929707\n"
     ]
    }
   ],
   "source": [
    "# filter methods - correlation\n",
    "run_randomForests(X_train_corr, X_test_corr, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3060c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8066004772684517\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7859521124929707\n"
     ]
    }
   ],
   "source": [
    "# filter methods - univariate roc-auc\n",
    "run_randomForests(X_train_corr, X_test_corr, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f94b7ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.825594244784318\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8037861254524954\n"
     ]
    }
   ],
   "source": [
    "# embedded methods - Random forests\n",
    "run_randomForests(X_train_rf, X_test_rf, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b2b0b",
   "metadata": {},
   "source": [
    "Random Forests built using 16 features display a slightly higher performance than a model built with all features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5c6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

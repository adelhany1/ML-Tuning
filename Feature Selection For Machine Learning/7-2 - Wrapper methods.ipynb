{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ab317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23bba045",
   "metadata": {},
   "source": [
    "# 07.3-Step-backward-feature-selection-mlxtend\n",
    "# 07.4-Step-backward-feature-selection-sklearn\n",
    "### Step backward feature selection\n",
    "\n",
    "Step Backward Feature Selection starts by fitting a model using all the features in the data set and determining its performance. \n",
    "\n",
    "Then, it trains models on all possible combinations of all features, minus one, and removes the feature that returns the model with the lowest performance.\n",
    "\n",
    "In the third step, it trains models in all possible combinations of the features remaining from step 2, minus 1 feature, and removes the feature that produced the lowest performing model.\n",
    "\n",
    "The algorithm stops when a certain criteria determined by the user is met. This criteria could be that the model performance does not decrease beyond a certain threshold, or alternatively, as we show in this notebook, when we reach a certain number of selected features.\n",
    "\n",
    "The evaluation metric can be the roc_auc for classification or the r squared for regression, for example, and is determined by the user.\n",
    "\n",
    "Step Backward Feature Selection is called greedy because it evaluates all possible n, and then n-1 and n-2 and so on feature combinations. Therefore, it is very computationally expensive and sometimes, if the feature space is big enough, even unfeasible.\n",
    "\n",
    "Scikit-learn provides various stopping criteria to stop the search:\n",
    "\n",
    "* when a certain number of features is reached (like MLXtend) (arbitrary)\n",
    "* if the performance does not increase beyond a certain threshold (ideal but expensive)\n",
    "* selects half of the features (arbitrary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0e1f24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SFS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 07.3-Step-backward-feature-selection-mlxtend\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# this is going to take a while, do not despair\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m sfs \u001b[38;5;241m=\u001b[39m \u001b[43mSFS\u001b[49m(RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     16\u001b[0m           k_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m65\u001b[39m, \u001b[38;5;66;03m# the lower the features we want, the longer this will take\u001b[39;00m\n\u001b[0;32m     17\u001b[0m           forward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, floating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m,  cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     20\u001b[0m sfs \u001b[38;5;241m=\u001b[39m sfs\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SFS' is not defined"
     ]
    }
   ],
   "source": [
    "# 07.3-Step-backward-feature-selection-mlxtend\n",
    "\n",
    "\n",
    "# within the SFS we indicate:\n",
    "# 1) the algorithm we want to create, in this case RandomForests (note that I use few trees to speed things up)\n",
    "# 2) the stopping criteria: want to select 50 features\n",
    "\n",
    "# 3) wheter to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the want cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(RandomForestClassifier(n_estimators=10, n_jobs=4, random_state=0),\n",
    "          k_features=65, # the lower the features we want, the longer this will take\n",
    "          forward=False, floating=False,\n",
    "          verbose=2, scoring='roc_auc',  cv=2)\n",
    "\n",
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816261d",
   "metadata": {},
   "source": [
    "# 07.4-Step-backward-feature-selection-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc92b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5860985",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262f2a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 109)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.53271</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.34991</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var_1     var_2      var_3     var_4    var_5     var_6     var_7  \\\n",
       "0  4.53271  3.280834  17.982476  4.404259  2.34991  0.603264  2.784655   \n",
       "\n",
       "      var_8      var_9    var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691  0.139346  ...  2.079066  6.748819  2.941445   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "\n",
       "[1 rows x 109 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset_2.csv')\n",
    "print(data.shape)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666960ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( data.drop(labels=['target'], axis=1),data['target'],test_size=0.3, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807ea08",
   "metadata": {},
   "source": [
    "### Remove correlated features\n",
    "\n",
    "Step Backward Feature Selection takes a long time to run, so to speed it up we will reduce the feature space by removing correlated features first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9096803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  36\n"
     ]
    }
   ],
   "source": [
    "# remove correlated features to reduce the feature space\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086cd625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 72), (15000, 72))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8624ee",
   "metadata": {},
   "source": [
    "### Step Backward Feature Selection\n",
    "\n",
    "For the Step Backward feature selection algorithm, we are going to use the class SFS from MLXtend:\n",
    "http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027f0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "# 1) the algorithm we want to create, in this case RandomForests (note that I use few trees to speed things up)\n",
    "# 2) the stopping criteria: check sklearn documentation for more details\n",
    "# 3) wheter to perform step forward or step backward\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the want cross-validation\n",
    "# this is going to take a while, do not despair\n",
    "sfs = SFS(estimator=RandomForestClassifier(n_estimators=10, n_jobs=4, random_state=0) ,\n",
    "    n_features_to_select=65 ,  # the number of features to retain\n",
    "    tol=None ,  # the maximum increase or decrease in the performance metric\n",
    "    direction='backward' ,  # the direction of the selection procedure\n",
    "    scoring='roc_auc' ,  # the metric to evaluate\n",
    "    cv=2 ,  # the cross-validation fold\n",
    "    n_jobs=4 ,  # for parallelization\n",
    ")\n",
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d07467",
   "metadata": {},
   "source": [
    "In the previous log, we see that the performance does not decrease, so we could continue removing features. If you have time and patience, why don't you try that?\n",
    "\n",
    "### Compare performance of feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730f6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train random forests and evaluate the performance\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d433cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7',\n",
       "       'var_8', 'var_10', 'var_11', 'var_12', 'var_13', 'var_14',\n",
       "       'var_15', 'var_16', 'var_18', 'var_19', 'var_20', 'var_21',\n",
       "       'var_22', 'var_23', 'var_25', 'var_26', 'var_27', 'var_30',\n",
       "       'var_31', 'var_34', 'var_35', 'var_37', 'var_40', 'var_41',\n",
       "       'var_45', 'var_47', 'var_48', 'var_49', 'var_50', 'var_51',\n",
       "       'var_52', 'var_53', 'var_55', 'var_56', 'var_58', 'var_60',\n",
       "       'var_62', 'var_63', 'var_67', 'var_68', 'var_69', 'var_71',\n",
       "       'var_73', 'var_77', 'var_78', 'var_79', 'var_81', 'var_82',\n",
       "       'var_83', 'var_86', 'var_89', 'var_90', 'var_91', 'var_93',\n",
       "       'var_96', 'var_98', 'var_103', 'var_107'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat= sfs.get_feature_names_out()\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6107f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7118554905228884\n",
      "Test set\n",
      "Random Forests roc-auc: 0.6960298383148206\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built using selected features\n",
    "run_randomForests(X_train[selected_feat], X_test[selected_feat], y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68121579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7119921185820277\n",
      "Test set\n",
      "Random Forests roc-auc: 0.6957598691250635\n"
     ]
    }
   ],
   "source": [
    "# and for comparison, we train random forests using\n",
    "# all features (except the correlated ones, which we removed already)\n",
    "run_randomForests(X_train,X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23cb456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance, as expected is roughly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1e64d",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Let's now repeat the process but in the context of regression. With the house prices dataset from Kaggle, the aim is to predict the continuous target: House Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "215dcbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "\n",
       "[1 rows x 81 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../houseprice.csv')\n",
    "print(data.shape)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c8a682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing, so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "# here for simplicity I will use only numerical variables select numerical columns:\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84811bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(labels=['SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d8e472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  3\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4c2eb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 34), (438, 34))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b152841",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01670f71",
   "metadata": {},
   "source": [
    "### Step Backward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b00816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step backward feature selection algorithm\n",
    "sfs = SFS( estimator = RandomForestRegressor(n_estimators=10, n_jobs=4, random_state=10), \n",
    "    n_features_to_select=20, # the number of features to retain\n",
    "    tol=None, # the maximum increase or decrease in the performance metric\n",
    "    direction='forward', # the direction of the selection procedure\n",
    "    scoring='r2', # the metric to evaluate\n",
    "    cv=2, # the cross-validation fold\n",
    "    n_jobs=4, # for parallelization\n",
    ")\n",
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "165e8ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
       "       'YearRemodAdd', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
       "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'Fireplaces', 'GarageCars', 'WoodDeckSF', '3SsnPorch',\n",
       "       'ScreenPorch', 'PoolArea'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2336e",
   "metadata": {},
   "source": [
    "### Compare performance of feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "693c84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train random forests and evaluate the performance\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestRegressor(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(r2_score(y_train, pred)))\n",
    "    print('Test set')\n",
    "    pred = rf.predict(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(r2_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b0a149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
       "       'YearRemodAdd', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
       "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'Fireplaces', 'GarageCars', 'WoodDeckSF', '3SsnPorch',\n",
       "       'ScreenPorch', 'PoolArea'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat= sfs.get_feature_names_out()\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c630eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8629621682457452\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8246015925536159\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built using selected features\n",
    "run_randomForests(X_train[selected_feat], X_test[selected_feat], y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ce99671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8699152317492538\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8190809813112794\n"
     ]
    }
   ],
   "source": [
    "# and for comparison, we train random forests using all features (except the correlated ones, which we removed already)\n",
    "run_randomForests(X_train,  X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2dc5c",
   "metadata": {},
   "source": [
    "*************************\n",
    "**************************\n",
    "************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52e4d4",
   "metadata": {},
   "source": [
    "# 07.5-Exhaustive-feature-selection\n",
    "### Exhaustive Feature Selection\n",
    "- Algorithm will evaluate all combinations and determine the best subset of features\n",
    "\n",
    "Exhaustive Feature Selection finds the best subset of features out of all possible subsets, according to a determined performance metric for a certain machine learning algorithm.\n",
    "\n",
    "For example, if we train a logistic regression and the dataset consists of 4 features, the algorithm will evaluate all **15** feature combinations as follows:\n",
    "\n",
    "- all possible combinations of 1 feature\n",
    "\n",
    "- all possible combinations of 2 features\n",
    "\n",
    "- all possible combinations of 3 features\n",
    "\n",
    "- all the 4 features\n",
    "\n",
    "and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression.\n",
    "\n",
    "Exhaustive Feature Selection is a greedy algorithm as it evaluates all possible feature combinations. It is very computationally expensive and, sometimes, if the feature space is large, even unfeasible.\n",
    "\n",
    "There is a special package for Python that implements this type of feature selection: mlxtend.\n",
    "\n",
    "http://rasbt.github.io/mlxtend/\n",
    "\n",
    "In the mlxtend implementation of the Exhaustive Feature Selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features. \n",
    "\n",
    "This is somewhat arbitrary, we might be selecting a sub-optimal number of features, or likewise, a high number of features. But, by looking at the performance metric returned by the algorithm as it selects the features, we can have a view on whether more features do add value, or not. \n",
    "\n",
    "**Note**\n",
    "\n",
    "If we wanted to stop the search by using another criteria, we would have to code the algorithm ourselves, unfortunately :(\n",
    "\n",
    "Here I will use the Step Forward feature selection algorithm from mlxtend in a classification and regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72a720e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f245c",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab16ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 109)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.53271</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.34991</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var_1     var_2      var_3     var_4    var_5     var_6     var_7  \\\n",
       "0  4.53271  3.280834  17.982476  4.404259  2.34991  0.603264  2.784655   \n",
       "\n",
       "      var_8      var_9    var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691  0.139346  ...  2.079066  6.748819  2.941445   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "\n",
       "[1 rows x 109 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# few rows to speed things up\n",
    "data = pd.read_csv('../dataset_2.csv', nrows=10000)\n",
    "print(data.shape)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07fca63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000, 108), (3000, 108))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( data.drop(labels=['target'], axis=1), data['target'], test_size=0.3, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c7912",
   "metadata": {},
   "source": [
    "### Remove correlated features\n",
    "\n",
    "The Exhaustive Feature Selection takes a long time to run, so to speed it up we will reduce the feature space by removing correlated features first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "096a1cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  59\n"
     ]
    }
   ],
   "source": [
    "# remove correlated features to reduce the feature space\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "# note that we reduce the correlation threshold to remove more features\n",
    "corr_features = correlation(X_train, 0.6)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0c0fc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000, 49), (3000, 49))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7f503",
   "metadata": {},
   "source": [
    "###  Exhaustive Feature Selection\n",
    "\n",
    "For the Exhaustive Feature Selection algorithm, we are going to use the class EFS from MLXtend:\n",
    "http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8619f3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 1225/1225"
     ]
    }
   ],
   "source": [
    "# in order to shorter search time for the demonstration i will ask the algorithm to try all possible 1 and 2 feature combinations\n",
    "# if you have access to a multicore or distributed computer system you can try more greedy searches\n",
    "###################################\n",
    "# within the EFS we indicate:\n",
    "# 1) the algorithm we want to create, in this case RandomForests (note that I use few trees to speed things up)\n",
    "# 2) the number of minimum features we want our model to have\n",
    "# 3) the number of maximum features we want our model to have\n",
    "# with 2 and 3 we regulate the number of possible feature combinations to be evaluated by the model.\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "# this is going to take a while, do not despair\n",
    "efs = EFS(RandomForestClassifier(n_estimators=5,  n_jobs=4,  random_state=0,  max_depth=2),\n",
    "          min_features=1,\n",
    "          max_features=2, scoring='roc_auc', print_progress=True, cv=2)\n",
    "# search features\n",
    "efs = efs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283c32a",
   "metadata": {},
   "source": [
    "The log above means that the search evaluated 1225 feature combinations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e41cb610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efs.best_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07089e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_16', 'var_55'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat = X_train.columns[list(efs.best_idx_)]\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79366751",
   "metadata": {},
   "source": [
    "### Compare performance of feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56f3dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train random forests and evaluate the performance\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e68fa78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7152616074556793\n",
      "Test set\n",
      "Random Forests roc-auc: 0.6868327155531925\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of classifier using selected features\n",
    "run_randomForests(X_train[selected_feat], X_test[selected_feat],  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4685a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7509464508443697\n",
      "Test set\n",
      "Random Forests roc-auc: 0.6874840935262856\n"
     ]
    }
   ],
   "source": [
    "# and for comparison, we train random forests using\n",
    "# all features (except the correlated ones, which we removed already)\n",
    "run_randomForests(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637d2e6",
   "metadata": {},
   "source": [
    "Even with 2 features, the performance is not super far off of that model built using all features!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e999303",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Let's now repeat the process but in the context of regression. With the house prices dataset from Kaggle, the aim is to predict the continuous target: House Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e1140ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../houseprice.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a15b94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing, so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "# here for simplicity I will use only numerical variables select numerical columns:\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea8e7f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( data.drop(labels=['SalePrice'], axis=1),  data['SalePrice'], test_size=0.3, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f24eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  9\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "# note thta we reduce the threshold to remove more features\n",
    "corr_features = correlation(X_train, 0.6)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea10ef92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 28), (438, 28))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b6c5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd1269",
   "metadata": {},
   "source": [
    "###  Exhaustive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d61104cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 406/406"
     ]
    }
   ],
   "source": [
    "# exhaustive search\n",
    "# in order to shorter search time for the demonstration i will ask the algorithm to try all possible 10 and 11 feature combinations\n",
    "# if you have access to a multicore or distributed computer system you can try more greedy searches\n",
    "efs = EFS(RandomForestRegressor(n_estimators=5, n_jobs=4, random_state=0, max_depth=2),\n",
    "          min_features=1,  max_features=2, scoring='r2',\n",
    "          print_progress=True, cv=2)\n",
    "efs = efs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a89f9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The searched evaluated 406 feature combinations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6ee9514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efs.best_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa05491d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OverallQual', 'BsmtFinSF1'], dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[list(efs.best_idx_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadf42f",
   "metadata": {},
   "source": [
    "### Compare performance of feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a189567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train random forests and evaluate the performance\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestRegressor(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(r2_score(y_train, pred)))\n",
    "    print('Test set')\n",
    "    pred = rf.predict(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(r2_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9416c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat = X_train.columns[list(efs.best_idx_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "245afc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7598962603686706\n",
      "Test set\n",
      "Random Forests roc-auc: 0.6856450799849569\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built using selected features\n",
    "run_randomForests(X_train[selected_feat], X_test[selected_feat],  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "031e0416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8454047044087043\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7806708079073277\n"
     ]
    }
   ],
   "source": [
    "# and for comparison, we train random forests using all features (except the correlated ones, which we removed already)\n",
    "run_randomForests(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7890b87a",
   "metadata": {},
   "source": [
    "In this case, the performance decreased quite a lot, so there are additional features that are good predictors of the target.\n",
    "\n",
    "The Exhaustive Search is extremely computationally expensive. I do not regularly use this selection procedure for this same reason. But if you have access to super computers, you may give it a go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9d6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
